# Local_RAG_Ollama
Its a local RAG system where it will retrive the data from the csv and answer the queries using the llm. It will run on any laptop the lowest we can go is no GPU and if you have GPU then it will basically be fast.

First Install OLLAMA on you system. Then we needed 2 models which are basically free to download from ollama
1-: Llama 3.2 
2-: nomic-embed-text  This is used for embedding of the text

Thats it now the code are given and there is a txt file to downlaod the necessary libraries.
